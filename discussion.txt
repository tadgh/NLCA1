3.1. Celebrity Potpourri:
After having generated the arff file (celebpotpour.arff) I executed the following commands to gather information about the differences between the classifiers:

java -cp weka.jar weka.classifiers.functions.SMO -t celebpotpour.arff -o
java -cp weka.jar weka.classifiers.bayes.NaiveBayes -t celebpotpour.arff -o
java -cp weka.jar weka.classifiers.trees.J48 -t celebpotpour.arff -o


Found in 3.1output.txt is the SMO data.

Seeing as by default, feeding it only a single arff file will cause it to do 10-fold cross-validation, we do not have to add any other parameters to these commands. 

To begin with, SMO, support vector machines, had accuracy of 50.52% during cross-validation. This roughly matched what it had for accuracy in the training data, though it is slightly
lower, as is logical given the fact that we are then testing on our training data.
Next up, naive bayes. In this case, we achieved 43.9% accuracy during cross-validation. This is worse than the SMO so far.
Finally, we look at trees.J48, which achieved 45.3%, which is extremely similar to the result we achieve with naive bayes.
However, it seems to be the support vector machines that allow us to achieve the best possible result. One interesting thing to note,
is that when I started including blank tweets (tweets which were empty after processing), SVM and bayes went down in accuracy, whereas J48 went up in accuracy.


3.2 Pop Stars.

Using the SVM method which previously generated the best data, we now have this from WEKA:

>>>java -cp weka.jar weka.classifiers.functions.SMO -t pop.arff -o

We can observe the results when using 10x cross-fold validation (see 3.2output_10xval.txt)
Compared to our results in the previous section, it appears as though the classifier is having more trouble distinguishing between these different tweeters.
This is logical, as since they are all from the same superclass "pop stars", it makes sense that their writing styles are similar given the features we are extracting.
When I decided to use the same training file to be the testing file, here are the results from WEKA (check 3.2output.txt).

>>>java -cp weka.jar weka.classifiers.functions.SMO -t pop.arff -T pop.arff -o

Using the same file for testing against, we achieve better performance, though only very marginally so.
I suppose this makes sense as we are using literally the exact same data that we built our model with,
it makes sense that it would perform mildly better, since we can include parts of the training data. Maybe a direction for
future work would be to look into better ways to attempt to sub-group various pop singers and thus extracting more relevant
features.

3.3 News

Correctly Classified Instances        2225               37.0833 %
Incorrectly Classified Instances      3775               62.9167 %

=== Confusion Matrix ===

               a   b   c   d   e   f   <-- classified as
             650  36 118  36  69  91 |   a = CBCNews
             292 159 160 135 152 102 |   b = cnn
             432  38 251  64  84 131 |   c = torontostarnews
             115 115  65 430  84 191 |   d = TheOnion
             334  76 145  99 220 126 |   e = Reuters
             183  40  46 128  88 515 |   f = nytimes
 TOTALS:    2006 464 785 892 697 1156


According to my results, it is in fact more difficult to distinguish different news agencies from one another compared to pop stars.
This indicates to me that pop stars are a more distinguishable and unique group when compared to newscasters as a whole.

Here we compute the precision and recall for each individual class to determine which is the most identifiable.
CBCNews: 		    Precision = 0.32403 Recall = 0.650
cnn: 			    Precision = 0.34267 Recall = 0.159
torontostarnews:	Precision = 0.31975 Recall = 0.251
TheOnion: 		    Precision = 0.48206 Recall = 0.430
Reuters: 		    Precision = 0.31564 Recall = 0.220
nytimes: 		    Precision = 0.44550 Recall = 0.515

Now we have to determine which of the news organizations is the most "distinct" from the others. In order to do this, I believe
that a good measure of distinctness is this: It's own tweets are mostly classified correctly, with few other tweets being mistaken for it.
Thus we must use some combination of recall and precision. 
For now, I will take an average of precision and recall and select the class producing the highest value.
In this case, the higest values are

nytimes:    0.48025
CBCNews:    0.48702
The Onion:  0.45603

However, given these extremely similar values that we get by taking an average, we can look at the individual values to make a decision.
CBC has extremely high recall, and low precision, whereas both TheOnion and the nytimes have very similar precision and recall scores.
For this reason I would have to then say that the CBCNews account is the most distinct.
Least Distinct is cnn at 0.25083, which apparently means that it is quite difficult to classify their own tweets correctly,
and other news organizations often get mistaken for them. Even looking at totals for the columns, cnn has 785 tweets classified as them,
with only 251 of them actually being cnn tweets.

3.4 Pop Versus News

In this question we split the classes into pop and news, each of which have 6 underlying .twt files to analyze from.

With full data (all 1000 tweets) we see correct classification percentage of 86.125%. When I remove tweets that are empty, it rises to 87.34%.

Here we can see what is obviously to be expected. Using 12000 tweets to classify, we end up with an extremely good level
of accuracy in the cross-validation. This is due to the fact that the pop and news group are very distinct when compared to each other. The
cross-validation in the other sections are comparing classes that are less distinct, and thus, the accuracy is lower.
Here we have the statistics for precision and recall for pop and news at full data.

Pop:    Precision = 0.90997 Recall = 0.80183
News:   Precision = 0.82288 Recall = 0.92067

Moving onto the half-data section, where we take only 500 tweets from each file, we achieve 86.83% correct classifications.
We notice that there is a slight drop in performance compared to the accuracy of the full data version. This is obvious, since the two classes
are distinct. Thus, more data allows us to better cluster the different classes, increasing the level of linear separability of the two. However,
it is surprising that cutting out half the data only marginally affected the performance. This is a strong indicator that the two classes
have a very high degree of linear separability that is only further enforced by the inclusion of more data. In tmers of the
fairness of the comparison, I believe it is absolutely fair. This is the point of a classifier, to distinguish two or more possible
classes. The classifier doesn't care what goes into your classes. This classifier is just as if not more useful than the previous ones,
as it provides us with a high degree of accuracy. I suppose when one references "unfairness" it is a measure of how obviously distinct
the classes are and the ease with which they are apparently classified. I personally feel that there is no unfairness. You could
compare tweets in english and german, and even if the accuracy is 100%, the classifier and the comparison are still fair and valid.
Here we see the values for the half-data 10X cross-fold validation:

Pop:    Precision = 0.90993 Recall = 0.79133
News:   Precision = 0.81539 Recall = 0.92167


3.5 Feature Analysis

After having analyzed the features on all attributes, I have come to a few small conclusions:
Firstly, average sentence length never drops below fifth rank in usefulness. Proper and common nouns are also consistently
useful in classification. I imagine this to be because every individual has a sentence length they tend to enjoy. Personally,
I know I trail on and on with many commas. In terms of commonalities that sit at the bottom of the list are Future Tense,
and wh-words. If I had to choose a single overriding feature that proves to be most overall useful given the current
arff files, I would choose Average Sentence Length.  One obvious comparison to make in terms of differences, is that the only
time that First Person breaks into the top five is in the Pop Versus News arff. The reason for this is obvious; News
organizations never refer to themselves by "I" or any other such pronoun. Thus the presence of such a personal pronoun
is a strong indicator that the tweet is pop-related. There are also other such features. Speaking of the news ARFF specifically,
the usage of proper nouns is the strongest indicator, at 0.11351. Looking at the bottom of the list are modern slang and future tense.
This makes perfect sense, seeing as essentially no news agency would use modern slang in its tweets, and also, news agencies tend
to report on what is currently happening, or what just happened. Rarely would they tweet about the future. I feel as though
these numbers all reflect reality.


3.6 Bonus: 
Here are the things I have done to improve the tokenizer and arffbuilder.
- Seriously added to modern slang. Many additions
- Fixed errors in tagging. Here are the ones I fixed:
	- i is incorrectly tagged as NN when it should be PRP.
	- I have elected to collapse repeated punctuation, as !!! is currently tagged as 'NN' instead of '.'

I would also like to do the second bonus section, wherein we discuss the possibility of using only five different data
points from both pop stars and news agencies. In this case, what I believe will happen is that we will either slightly
increase or slightly decrease accuracy, depending on which data set is withheld. My reasoning for this is that given
our extremely small data size, it is possible that a large portion of tweets have very little of value to add. Thus,
omitting 20% of the data at this size could cause a reasonable swing in accuracy. We will now test a few scenarios:

1. Omit The Onion and omit Rihanna.
2. Omit The Onion and omit Justin Bieber.
3. Omit CNN and Rihanna
4. Omit CNN and Justin Bieber.

This may give us some insight as to which tweeter is more influential within their individual class.

1.
Correctly Classified Instances        8781               87.81   %
Incorrectly Classified Instances      1219               12.19   %

2.
Correctly Classified Instances        8693               86.93   %
Incorrectly Classified Instances      1307               13.07   %

3.
Correctly Classified Instances        8726               87.26   %
Incorrectly Classified Instances      1274               12.74   %

4.
Correctly Classified Instances        8667               86.67   %
Incorrectly Classified Instances      1333               13.33   %


As predicted, there were marginal shifts in both directions, but nothing extremely large. First we will discuss the effect
of adding or removing a particular user. Take example 1, where we removed Rihanna and TheOnion. When we did this, classification
rates rose just over 1% compared to using all of the data. When we compare this to example 2 where we get a much smaller increase,
this is indicative to me that justin bieber's tweets are more useful in classification than rihanna's are. Same goes for
CNN and the onion. When removing cnn, we get slightly worse results than when we remove the onion, indicating that cnn has
a larger impact on classification than the onion does.
On a larger level, the fact that we can remove 20% of the data without experiencing extremely large shifts in accuracy
is a good indication that the groups themselves are in fact quite distinct, and the removal of a single tweeter does not
adversely effect classification too much.